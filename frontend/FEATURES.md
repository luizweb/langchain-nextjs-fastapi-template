# ğŸš€ SeleÃ§Ã£o de Modelo LLM - Guia de Funcionalidade

## ğŸ“‹ VisÃ£o Geral

Implementamos um sistema completo de seleÃ§Ã£o de modelos LLM que permite aos usuÃ¡rios escolher qual modelo de inteligÃªncia artificial desejam usar no chat, com suporte a mÃºltiplos provedores.

## âœ¨ Funcionalidades

### 1. **SeleÃ§Ã£o DinÃ¢mica de Providers**
- âœ… Carregamento automÃ¡tico dos providers disponÃ­veis via API
- âœ… Suporte a Ollama, OpenAI e Serpro
- âœ… Interface visual com badges coloridos por provider

### 2. **SeleÃ§Ã£o de Modelos**
- âœ… Lista dinÃ¢mica de modelos baseada no provider selecionado
- âœ… Display em fonte monoespaÃ§ada para melhor legibilidade
- âœ… Contador de modelos disponÃ­veis por provider

### 3. **PersistÃªncia de PreferÃªncias**
- âœ… SeleÃ§Ã£o salva automaticamente no localStorage
- âœ… PreferÃªncias restauradas ao recarregar a pÃ¡gina
- âœ… Feedback visual de salvamento

### 4. **IntegraÃ§Ã£o com Chat**
- âœ… Modelo enviado em cada requisiÃ§Ã£o de chat
- âœ… Badge identificador do modelo usado em cada resposta
- âœ… Compatibilidade total com streaming SSE

### 5. **UX Aprimorada**
- âœ… Loading states durante carregamento de providers
- âœ… Error handling com mensagens claras
- âœ… Design responsivo e moderno
- âœ… Ãcones intuitivos (Bot, Sparkles)

## ğŸ¨ Interface do UsuÃ¡rio

### Componente LLMSelector

O componente estÃ¡ localizado na sidebar esquerda da pÃ¡gina de chat, acima da Ã¡rea de upload de PDFs.

**Elementos visuais:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ¤– Modelo LLM âœ¨                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Provider                            â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ [Ollama] (4 modelos)            â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                     â”‚
â”‚ Modelo                              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ gpt-oss:120b-cloud              â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ¨ PreferÃªncias salvas             â”‚
â”‚    automaticamente                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Cores dos Providers

- **Ollama**: Azul (`blue-500`)
- **OpenAI**: Verde (`green-500`)
- **Serpro**: Roxo (`purple-500`)

### Badge nas Mensagens

Cada resposta do assistente exibe um badge mostrando qual modelo foi usado:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [ollama/gpt-oss:120b-cloud]       â”‚
â”‚                                    â”‚
â”‚ [Resposta do assistente...]       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ ImplementaÃ§Ã£o TÃ©cnica

### Arquitetura

```
Frontend (React/Next.js)
â”‚
â”œâ”€â”€ components/llm-selector.tsx
â”‚   â”œâ”€â”€ Fetch providers da API
â”‚   â”œâ”€â”€ Gerencia estado local
â”‚   â”œâ”€â”€ Salva no localStorage
â”‚   â””â”€â”€ Notifica componente pai
â”‚
â””â”€â”€ app/projects/chat/page.tsx
    â”œâ”€â”€ Recebe seleÃ§Ã£o via callback
    â”œâ”€â”€ Envia provider/model na requisiÃ§Ã£o
    â””â”€â”€ Exibe badge do modelo usado
```

### Fluxo de Dados

1. **Montagem do Componente**
   ```typescript
   useEffect(() => {
     fetchProviders() â†’ API GET /chat/providers
     â†“
     Carrega preferÃªncias do localStorage
     â†“
     Inicializa com default ou preferÃªncia salva
     â†“
     onSelectionChange({ provider, model })
   })
   ```

2. **MudanÃ§a de SeleÃ§Ã£o**
   ```typescript
   handleProviderChange(provider)
   â†“
   Auto-seleciona primeiro modelo
   â†“
   Salva no localStorage
   â†“
   onSelectionChange({ provider, model })
   ```

3. **Envio de Mensagem**
   ```typescript
   sendMessage()
   â†“
   POST /chat/stream
   {
     query: "...",
     project_id: 123,
     provider: "ollama",    â† Adicionado
     model: "gpt-oss:..."   â† Adicionado
   }
   ```

## ğŸ“¦ Estrutura de Dados

### API Response - GET /chat/providers

```typescript
interface LLMProvidersResponse {
  providers: Array<{
    name: string;
    models: string[];
  }>;
  default_provider: string;
  default_model: string;
}

// Exemplo:
{
  "providers": [
    {
      "name": "ollama",
      "models": ["mistral", "llama2", "codellama", "gpt-oss:120b-cloud"]
    },
    {
      "name": "openai",
      "models": ["gpt-4o", "gpt-4o-mini", "gpt-4-turbo", "gpt-3.5-turbo"]
    },
    {
      "name": "serpro",
      "models": ["gpt-oss-120b", "deepseek-r1-distill-qwen-14b"]
    }
  ],
  "default_provider": "ollama",
  "default_model": "gpt-oss:120b-cloud"
}
```

### Chat Request - POST /chat/stream

```typescript
interface ChatRequest {
  query: string;
  project_id: number;
  provider: string;  // Novo campo
  model: string;     // Novo campo
}
```

### Message Type

```typescript
interface ChatMessage {
  role: "user" | "assistant";
  content: string;
  toolCalls?: ToolCall[];
  toolResults?: ToolResult[];
  llmProvider?: string;  // Novo campo
  llmModel?: string;     // Novo campo
}
```

## ğŸ¯ Casos de Uso

### Caso 1: Primeiro Acesso
1. UsuÃ¡rio acessa a pÃ¡gina de chat
2. LLMSelector carrega providers da API
3. Usa provider/modelo padrÃ£o (ollama/gpt-oss:120b-cloud)
4. Salva preferÃªncia no localStorage
5. UsuÃ¡rio envia mensagem â†’ usa modelo padrÃ£o

### Caso 2: MudanÃ§a de Provider
1. UsuÃ¡rio clica no select de Provider
2. Escolhe "OpenAI"
3. Modelo Ã© auto-selecionado para "gpt-4o" (primeiro da lista)
4. PreferÃªncia salva automaticamente
5. PrÃ³xima mensagem usa OpenAI/gpt-4o

### Caso 3: Retorno Ã  PÃ¡gina
1. UsuÃ¡rio retorna ao chat depois
2. LLMSelector restaura preferÃªncias do localStorage
3. MantÃ©m OpenAI/gpt-4o selecionado
4. Continua conversaÃ§Ã£o com mesmo modelo

### Caso 4: Modelo EspecÃ­fico
1. UsuÃ¡rio quer testar gpt-3.5-turbo
2. Seleciona OpenAI como provider
3. Seleciona gpt-3.5-turbo no dropdown de modelos
4. Envia mensagem de teste
5. Badge na resposta confirma: "openai/gpt-3.5-turbo"

## ğŸ’¡ BenefÃ­cios para UX

1. **TransparÃªncia**: UsuÃ¡rio sempre sabe qual modelo estÃ¡ sendo usado
2. **Controle**: Poder de escolha entre diferentes modelos
3. **ConveniÃªncia**: PreferÃªncias salvas automaticamente
4. **Feedback**: Visual claro do modelo em cada resposta
5. **Flexibilidade**: FÃ¡cil troca entre modelos durante a conversa

## ğŸ”’ SeguranÃ§a

- âœ… Token JWT validado em todas as requisiÃ§Ãµes
- âœ… PreferÃªncias salvas apenas localmente (nÃ£o no servidor)
- âœ… ValidaÃ§Ã£o de provider/modelo no backend
- âœ… Error handling para providers invÃ¡lidos

## ğŸ“± Responsividade

O componente Ã© totalmente responsivo e se adapta a diferentes tamanhos de tela:

- **Desktop**: Sidebar fixa de 288px (w-72)
- **Tablet**: Layout ajustado com scroll
- **Mobile**: Componente mantÃ©m usabilidade

## ğŸš€ Performance

- âš¡ Carregamento Ãºnico na montagem do componente
- âš¡ Cache de providers no estado local
- âš¡ localStorage para evitar requests repetidas
- âš¡ Lazy loading do componente via dynamic import (possÃ­vel)

## ğŸ§ª Testando a Funcionalidade

### Teste Manual

1. **Acesse a pÃ¡gina de chat:**
   ```
   http://localhost:3000/projects/chat?id=1
   ```

2. **Verifique o seletor na sidebar:**
   - Deve exibir providers disponÃ­veis
   - Badge colorido para cada provider
   - Contador de modelos

3. **Troque o provider:**
   - Selecione OpenAI
   - Observe mudanÃ§a automÃ¡tica do modelo
   - Mensagem de confirmaÃ§Ã£o

4. **Envie uma mensagem:**
   - Digite qualquer pergunta
   - Observe badge na resposta: "openai/gpt-4o"

5. **Recarregue a pÃ¡gina:**
   - SeleÃ§Ã£o deve ser mantida
   - Mesmo provider/modelo selecionado

### Teste de API

```bash
# 1. Listar providers disponÃ­veis
curl http://localhost:8000/chat/providers \
  -H "Authorization: Bearer YOUR_TOKEN"

# 2. Enviar mensagem com modelo especÃ­fico
curl -X POST http://localhost:8000/chat/stream \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "OlÃ¡!",
    "project_id": 1,
    "provider": "openai",
    "model": "gpt-4o"
  }'
```

## ğŸ“ PrÃ³ximas Melhorias (SugestÃµes)

1. **Favoritos**: Marcar modelos favoritos
2. **HistÃ³rico**: Mostrar Ãºltimos modelos usados
3. **ComparaÃ§Ã£o**: Chat side-by-side com modelos diferentes
4. **MÃ©tricas**: Exibir tempo de resposta por modelo
5. **Custos**: Mostrar estimativa de custo (OpenAI)
6. **Compartilhamento**: Compartilhar chat com modelo especÃ­fico
7. **Templates**: Criar presets de modelo por tipo de tarefa

## ğŸ“ Aprendizados

Esta implementaÃ§Ã£o demonstra:

- âœ… IntegraÃ§Ã£o completa frontend-backend
- âœ… Gerenciamento de estado em React
- âœ… PersistÃªncia local com localStorage
- âœ… Design system com shadcn/ui
- âœ… TypeScript para type-safety
- âœ… UX patterns para seleÃ§Ãµes dinÃ¢micas
- âœ… Error handling robusto
- âœ… Feedback visual contÃ­nuo

---

**Desenvolvido com â¤ï¸ usando Next.js 16, React 19 e shadcn/ui**
